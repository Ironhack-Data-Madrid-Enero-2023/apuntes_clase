{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "817c2e27-db63-4bd7-9982-9bccb1f52943",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# RDD creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a1e03a28-cb2e-42e5-96a8-1eb139149515",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### [Introduction to Spark with Python, by Jose A. Dianes](https://github.com/jadianes/spark-py-notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "efbb9d07-8604-45c5-913f-dee15eb1344f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Apache Spark trabaja con un conjunto de datos denominados RDD (Resilient Distributed Dataset o Conjunto de Datos Distribuidos Resistentes), estos poseen una serie de características que los hacen diferenciarse de otros tipos de estructuras de datos:\n",
    "  + Inmutables: Una vez creados no se pueden modificar.\n",
    "  + Distribuidos: Hace referencia al RDD, están divididos en particiones que están repartidas por el clúster\n",
    "  + Resilientes: Esto quiere decir que en el caso de que se pierda una partición, esta se regenara automáticamente.\n",
    "\n",
    "Los RDD a pesar de ser inmutables pueden ser transformados, de manera que se crean un nuevo RDD y estas transformaciones se aplican a los datos del nuevo RDD.\n",
    "\n",
    "Existen distintas formas de generar RDDs:\n",
    "  + A partir de un fichero\n",
    "  + Distribución de datos desde el driver\n",
    "  + Transformar un RDD para crear un nuevo RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "65301049-42cb-4610-9244-cc0e53e77fb7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Ciclo de vida de un RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9e87183-75f0-4276-b712-2a4501077a59",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![ciclo de vida de RDD](https://keepcoding.io/wp-content/uploads/2022/06/image-39-1024x473.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "267805e4-d2fe-43f5-9b8f-6ccba97477a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d5256e9a-c21a-4309-a20f-319dce8cd22d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "SparkContext o Punto de acceso. \n",
    "\n",
    "Para realizar operaciones necesitamos un Context: \n",
    "  + SparkContext, SQLContext...\n",
    "\n",
    "Dependerá del tipo de operación al principio estaba SparkContext y se usaba para operaciones con RDDs, despues salio SparkSession, para RDDs, Dataframes y Datsets. \n",
    "\n",
    "SparkSession contempla internamente el SparkContext, HiveContext, SQLContext...\n",
    "\n",
    "SparkSession nos sirve para todos los contextos.\n",
    "\n",
    "En principio usar SparkSession sería lo más correcto, ya que establecemos una sesión con el nodo maestro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b91fa49-fe77-4931-8f5f-a02e604169ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "sc=spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d322ef79-172d-45a5-b1da-340acf893f23",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "En SparkContext disponemos de una interfaz gráfica donde se encuentran todos los jobs ejecutados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec66f1ee-d13e-4e40-989e-895f5abbfa94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "410c2c28-941f-46af-b59b-5e08098baa2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Obtención de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c6e2b19d-552e-4a4f-99e8-0314282754f8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vamos a usar una parte (10%) del dataset utilizado en la KDD Cup de 1999, contiene aproximadamente medio millón de filas. La vamos a descargar de forma local en un archivo Gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "310aa6fb-5327-42e6-bbbb-ecb8dfd83f48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Crear un RDD desde un fichero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ff85a77-3b5e-4b2d-86ab-1b5d61f691b1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz\"\n",
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addFile(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "453500e0-2595-4e8f-a61d-fce1d6bd2d8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "myRDD = sc.textFile(\"file://\"+SparkFiles.get(\"kddcup.data_10_percent.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "acda18e5-6e28-4569-abff-1056c617a277",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(myRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8c984ed0-b10c-4031-af15-b0f395d4cf31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cd85f812-2592-4289-b436-a117f0a554df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "myRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "350a892b-b0b5-44f3-88d4-972cba9fc1cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Crear un RDD usando paralelización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "55aada38-a5ec-4282-b1d6-431f3e8eb4fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "a = range(100)\n",
    "\n",
    "data = sc.parallelize(a)\n",
    "type(data)\n",
    "\n",
    "'''\n",
    "Hace p particiones, tb se lo podemos especificar\n",
    "data = sc.parallelize(a, p)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "da16c185-daf9-4278-bda6-3379c59131c8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cdc2d4de-9b2e-4127-bc8e-dd688c4dcd38",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###Obtener datos y particiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e9a0386e-9fd3-4c34-aeaa-b686f8b8cfc2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Usaremos el método collect para obtener datos de un dataframe previamente filtrado ya, de esta forma evitamos tratar de cargar datos de forma masiva, lo que podría llevar a errores por falta de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a5f46ff-d25a-446e-80fd-754236027a6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "rddCollect = data.collect()\n",
    "\n",
    "print(\"Number of Partitions: \"+str(data.getNumPartitions()))\n",
    "print(\"Action: First element: \"+str(data.first()))\n",
    "print(rddCollect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "32a3c3cc-67da-4fde-a580-ce9fe46d8670",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Transformaciones de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1a49aac-fa04-4ad2-9450-b877d807c12e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "53645c4c-4035-4e73-ae11-96f165777cc7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Con esta tipo de transformación podemos generar un RDD con los elementos que cumplan una condición determinada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a98a4ad7-d607-4d8d-ac11-c507ef4f560b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "72d70625-284d-4cb9-b1f5-22c9e6f2d243",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Vamos a filtrar nuestro RDD para contar cuantas conexiones normales hay en el dataset\n",
    "'''\n",
    "normal_myRDD = myRDD.filter(lambda x: 'normal.' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "45533bd9-cfee-4855-a4d7-2d24bda95034",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "normal_myRDD.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bb0e2daa-de88-469c-bc84-963a867fed03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Como vemos el número total de filas ha cambiado de nuestro dataset original que tenía 494021 filas a 97278."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af2dbe02-c3ed-4d91-b179-467c8a32a610",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Como norma general los calculos se realizan cuando ejecutamos una acción, y no cuando realizamos transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e63e0c43-217a-49b9-9745-b8efde4263a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "t0 = time()\n",
    "normal_count = normal_myRDD.count()\n",
    "tt = time() - t0\n",
    "print(\"Hay {} interaciones etiquetadas como 'normal'\".format(normal_count))\n",
    "print(\"Count completedo en {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1f3133ab-aab6-4d2f-a8b0-513e95510530",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5eda0b9f-1b56-4a05-afc9-0e0e3560d708",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Al igual que en Python la función Map nos permite aplicar una función a cada elemento de nuestro RDD.\n",
    "Las expresiones lambda son muy útiles para este tipo de transformaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3d9a7f1f-b618-421a-8556-7edf5d92f246",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_data = myRDD.map(lambda x: x.split(\",\"))\n",
    "\n",
    "t0 = time()\n",
    "head_rows = csv_data.take(5)\n",
    "tt = time() - t0\n",
    "print(\"Completedo en {} segundos\".format(round(tt,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb19f503-99e0-4cb7-acee-b118d4a455e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "head_rows = csv_data.take(100000)\n",
    "tt = time() - t0\n",
    "print(\"Completedo en {} segundos\".format(round(tt,3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "379619ad-954f-4264-b841-f77637e5316f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_interaction(line):\n",
    "    elems = line.split(\",\")\n",
    "    tag = elems[41]\n",
    "    return {tag: elems[:-1]}\n",
    "\n",
    "key_csv_data = myRDD.map(parse_interaction)\n",
    "head_rows = key_csv_data.take(5)\n",
    "print(head_rows[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2927aab7-e80c-4431-8b6f-1e7bf71209a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(head_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d103a3b4-db1e-44ba-801b-9806bfebc126",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### The Collect action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad48c0b9-df0a-4907-9a9b-a327bd9cd133",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Collect es otro método de los báscicos que debemos conocer, básicamente lo que hace es coger todos los elementos de un RDD y los carga en nuestra memoria, para poder trabajar con ellos de forma local. \n",
    "\n",
    "Debemos ser precavidos cuando lo usemos, especialmente cuando estemos trabajando con RDDs con un gran número de filas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bba70e7d-a1e7-4043-9a57-4f7d3ab55169",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "t0 = time()\n",
    "all_raw_data = myRDD.collect()\n",
    "tt = time() - t0\n",
    "print(\"Data collected in {} seconds\".format(round(tt,3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f65f1efa-388d-4054-95a4-143d6f32768a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "all_raw_data[:3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d425fa5-f0d4-49ea-bb0b-97dec3797224",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Conceptos Básicos de Spark DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "54caa7ef-787f-4de0-9ae4-00e179ea2263",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Vamos a cargar unos datos desde la base de datos spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6630b347-b548-4db5-9c64-65962073127f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.json('dbfs:/databricks-datasets/structured-streaming/events/file-0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71e44e42-7ef2-4782-9186-797748138105",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57075ec2-6314-4b84-a964-64914ae34dec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Mostrar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8c77a427-3c2d-46a9-8002-fed2c80ed10a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7acfe52a-a27a-47fe-8c5b-56faab801ac8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f4d8d550-ce57-4513-ad8c-da035378d40c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "548acfdf-fbcb-4403-8f1f-d6a9f6654295",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6feca703-676c-48e7-af32-e634ed0fbc87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Operaciones con columnas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba14484a-7afe-4116-82f6-d13a206ca5d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Crear una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec525849-4f0b-4a8a-a512-fc98ab7b9e87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fd427278-e374-49fb-9b7a-c7da9df9a4da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "type(df['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c14a74c-8066-4a2f-9c29-1cbd2d5d04fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Para mostrar la columna como un dataframe\n",
    "'''\n",
    "df.select('time').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cc0f7235-b2bc-4ff1-9dfc-f898b26dbb41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.select('time'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f817b89-71ab-4aa8-a28f-702a1cbfb76e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e8967523-488f-4667-8356-12b0b2c4f0cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.select(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ed01f7f2-af0c-41b0-8b42-eaa40a623dac",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.select('time','action'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a0df8992-a15d-4a20-9864-89ed90838989",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Renombrar una columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c66b04f-24ad-426a-9212-e6f092cb9d90",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.withColumnRenamed('action','superaction'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "50ced3e5-586c-4149-94df-d8879fab2c39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Operaciones con columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "45c98cab-1426-4763-93c9-abe53225ef96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.withColumn('newtime', df['time']+5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "05bee5f5-d90b-43b2-b9ed-b4e7ed08c8d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df.withColumn('half_time', df['time']/2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ceb9dff-e4e9-46f0-bf8d-857168d9cee1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "879f7729-2b33-4c7f-81a1-d752f3672b2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Podemos usar queries de SQL directamente sobre el dataframe, pero para ellos deberemos de guardarlo en una vista temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3108f78-9769-4a0f-b4f7-c0b290480ac8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"IoT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2d7bb7c0-119c-495b-bad0-d5a291f14b38",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql_results = spark.sql(\"SELECT * FROM IoT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a24654a7-2435-4ffc-bde3-f6dc43fc42b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(sql_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7138d15c-0726-4a71-81e9-9f78cdd60827",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT * From IoT WHERE time < 1469501149\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc929e2a-b640-4b94-bfec-9460544223eb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Documentación\n",
    "\n",
    "### [Ejemplos y Documentación Spark](https://sparkbyexamples.com/) \n",
    "\n",
    "### [Ejemplos y Documentación PySpark](https://sparkbyexamples.com/pyspark-tutorial/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cceef193-a10e-4f36-a798-745ca660138f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Workshop Databricks BT IH Ago 22",
   "notebookOrigID": 2110397458328224,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
